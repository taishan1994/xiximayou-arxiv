{"2024-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18804v1","updated":"2024-03-27T17:50:00Z","published":"2024-03-27T17:50:00Z","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation","summary":"  The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2403.18804v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18803v1","updated":"2024-03-27T17:49:31Z","published":"2024-03-27T17:49:31Z","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models","summary":"  Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Daniel Gillis","Judi McCuaig"],"pdf_url":"https://arxiv.org/pdf/2403.18803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2403.18783v1","updated":"2024-03-27T17:31:39Z","published":"2024-03-27T17:31:39Z","title":"Towards a World-English Language Model for On-Device Virtual Assistants","summary":"  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.\n","authors":["Rricha Jalota","Lyan Verwimp","Markus Nussbaum-Thom","Amr Mousa","Arturo Argueta","Youssef Oualil"],"pdf_url":"https://arxiv.org/pdf/2403.18783v1.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v1","updated":"2024-03-27T17:20:39Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: Robust Evaluation Framework using Large Language Model via\n  Checklist","summary":"  We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2403.18771v1.pdf","comment":"HEAL at CHI 2024"},{"id":"http://arxiv.org/abs/2403.18769v1","updated":"2024-03-27T17:13:38Z","published":"2024-03-27T17:13:38Z","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","summary":"  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n","authors":["Liang Lu","Jingzhi Wang","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2403.18769v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18746v1","updated":"2024-03-27T16:45:02Z","published":"2024-03-27T16:45:02Z","title":"CYCLE: Learning to Self-Refine the Code Generation","summary":"  Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.\n","authors":["Yangruibo Ding","Marcus J. Min","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2403.18746v1.pdf","comment":"Camera-ready for OOPSLA'24"},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18697v1","updated":"2024-03-27T15:46:25Z","published":"2024-03-27T15:46:25Z","title":"The Invalsi Benchmark: measuring Language Models Mathematical and\n  Language understanding in Italian","summary":"  While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.\n","authors":["Andrea Esuli","Giovanni Puccetti"],"pdf_url":"https://arxiv.org/pdf/2403.18697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.11128v2","updated":"2024-03-27T15:22:53Z","published":"2024-03-17T07:34:12Z","title":"Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities","summary":"  With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.\n","authors":["Honglin Mu","Yang Xu","Yunlong Feng","Xiaofeng Han","Yitong Li","Yutai Hou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.11128v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13320v2","updated":"2024-03-27T14:57:29Z","published":"2023-09-23T09:35:55Z","title":"GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification","summary":"  We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2309.13320v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18647v1","updated":"2024-03-27T14:54:27Z","published":"2024-03-27T14:54:27Z","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens","summary":"  We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.\n","authors":["Chengbo Liu","Yong Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18647v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.04507v2","updated":"2024-03-27T14:50:56Z","published":"2024-03-07T14:07:00Z","title":"NLPre: a revised approach towards language-centric benchmarking of\n  Natural Language Preprocessing systems","summary":"  With the advancements of transformer-based architectures, we observe the rise\nof natural language preprocessing (NLPre) tools capable of solving preliminary\nNLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or\nmorphological analysis) without any external linguistic guidance. It is arduous\nto compare novel solutions to well-entrenched preprocessing toolkits, relying\non rule-based morphological analysers or dictionaries. Aware of the\nshortcomings of existing NLPre evaluation approaches, we investigate a novel\nmethod of reliable and fair evaluation and performance reporting. Inspired by\nthe GLUE benchmark, the proposed language-centric benchmarking system enables\ncomprehensive ongoing evaluation of multiple NLPre tools, while credibly\ntracking their performance. The prototype application is configured for Polish\nand integrated with the thoroughly assembled NLPre-PL benchmark. Based on this\nbenchmark, we conduct an extensive evaluation of a variety of Polish NLPre\nsystems. To facilitate the construction of benchmarking environments for other\nlanguages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full\ncustomization of the publicly released source code of the benchmarking system.\nThe links to all the resources (deployed platforms, source code, trained\nmodels, datasets etc.) can be found on the project website:\nhttps://sites.google.com/view/nlpre-benchmark.\n","authors":["Martyna Wiącek","Piotr Rybak","Łukasz Pszenny","Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2403.04507v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18624v1","updated":"2024-03-27T14:34:29Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18609v1","updated":"2024-03-27T14:26:41Z","published":"2024-03-27T14:26:41Z","title":"A survey on learning models of spiking neural membrane systems and\n  spiking neural networks","summary":"  Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.\n","authors":["Prithwineel Paul","Petr Sosik","Lucie Ciencialova"],"pdf_url":"https://arxiv.org/pdf/2403.18609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2308.12531v2","updated":"2024-03-27T13:46:37Z","published":"2023-08-24T03:40:54Z","title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction","summary":"  Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.\n","authors":["Wenjun Kong","Yamei Xia"],"pdf_url":"https://arxiv.org/pdf/2308.12531v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18555v1","updated":"2024-03-27T13:34:59Z","published":"2024-03-27T13:34:59Z","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","summary":"  Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.\n","authors":["Philip Kenneweg","Sarah Schröder","Alexander Schulz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v3","updated":"2024-03-27T13:29:31Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v3.pdf","comment":"Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18504v1","updated":"2024-03-27T12:33:42Z","published":"2024-03-27T12:33:42Z","title":"AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA","summary":"  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n","authors":["Felix Virgo","Fei Cheng","Lis Kanashiro Pereira","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2403.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03544v2","updated":"2024-03-27T10:53:42Z","published":"2023-04-07T08:49:43Z","title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual\n  Topic Modeling","summary":"  Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n","authors":["Xiaobao Wu","Xinshuai Dong","Thong Nguyen","Chaoqun Liu","Liangming Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2304.03544v2.pdf","comment":"Accepted to AAAI2023 conference. Code is available at\n  https://github.com/BobXWu/InfoCTM"},{"id":"http://arxiv.org/abs/2309.13322v2","updated":"2024-03-27T10:50:24Z","published":"2023-09-23T09:51:37Z","title":"From Text to Source: Results in Detecting Large Language Model-Generated\n  Content","summary":"  The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2309.13322v2.pdf","comment":"Accepted to COLING-LREC 2024"},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18430v1","updated":"2024-03-27T10:36:17Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.18426v1","updated":"2024-03-27T10:27:28Z","published":"2024-03-27T10:27:28Z","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","summary":"  Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.\n","authors":["Jamshid Mozafari","Anubhav Jangra","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.18426v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17647v2","updated":"2024-03-27T10:07:59Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2403.18365v1","updated":"2024-03-27T08:57:21Z","published":"2024-03-27T08:57:21Z","title":"BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models","summary":"  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.\n","authors":["Haitao Li","Qingyao Ai","Jia Chen","Qian Dong","Zhijing Wu","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18365v1.pdf","comment":"11pages"},{"id":"http://arxiv.org/abs/2307.16071v2","updated":"2024-03-27T08:56:01Z","published":"2023-07-29T20:42:50Z","title":"ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus","summary":"  We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.\n","authors":["Tolulope Ogunremi","Kola Tubosun","Anuoluwapo Aremu","Iroro Orife","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2307.16071v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18350v1","updated":"2024-03-27T08:42:31Z","published":"2024-03-27T08:42:31Z","title":"Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language","summary":"  The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).\n","authors":["Ali Mahboub","Muhy Eddin Za'ter","Bashar Alfrou","Yazan Estaitia","Adnan Jaljuli","Asma Hakouz"],"pdf_url":"https://arxiv.org/pdf/2403.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v1","updated":"2024-03-27T08:39:56Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Da Ma","Situo Zhang","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18341v1","updated":"2024-03-27T08:32:19Z","published":"2024-03-27T08:32:19Z","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","summary":"  With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.\n","authors":["Xiusi Chen","Hongzhi Wen","Sreyashi Nag","Chen Luo","Qingyu Yin","Ruirui Li","Zheng Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18341v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11399v2","updated":"2024-03-27T07:05:22Z","published":"2024-03-18T01:14:47Z","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","summary":"  The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.\n","authors":["Dongjae Shin","Hyunseok Lim","Inho Won","Changsu Choi","Minjun Kim","Seungwoo Song","Hangyeol Yoo","Sangmin Kim","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.11399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12468v3","updated":"2024-03-27T06:46:56Z","published":"2023-02-24T05:48:53Z","title":"Adapting Knowledge for Few-shot Table-to-Text Generation","summary":"  Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.\n","authors":["Zhixin Guo","Minyxuan Yan","Jiexing Qi","Jianping Zhou","Ziwei He","Guanjie Zheng","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2302.12468v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04415"},{"id":"http://arxiv.org/abs/2403.18295v1","updated":"2024-03-27T06:43:58Z","published":"2024-03-27T06:43:58Z","title":"Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning","summary":"  Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.\n","authors":["Yongwei Zhou","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06201v3","updated":"2024-03-27T06:31:42Z","published":"2024-01-11T15:45:11Z","title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","summary":"  To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.\n","authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2401.06201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18277v1","updated":"2024-03-27T06:13:04Z","published":"2024-03-27T06:13:04Z","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","summary":"  Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.\n","authors":["Yejin Yoon","Jungyeon Lee","Kangsan Kim","Chanhee Park","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18277v1.pdf","comment":"Accepted to LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17636v2","updated":"2024-03-27T05:55:35Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.08590v2","updated":"2024-03-27T05:53:58Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18253v1","updated":"2024-03-27T04:51:42Z","published":"2024-03-27T04:51:42Z","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","summary":"  Metaphors are ubiquitous in daily life, yet detecting them poses a\nsignificant challenge. Previous approaches often struggled with improper\napplication of language rules and overlooked the issue of data sparsity. To\naddress these challenges, we introduce knowledge distillation and prompt\nlearning into metaphor detection. Specifically, we devise a prompt learning\ntemplate tailored for the metaphor detection task. By masking target words and\nproviding relevant prompt information, we guide the model to accurately infer\nthe contextual meaning of these words. This approach not only mitigates the\ninterference from the literal meaning of target words but also ensures the\nproper utilization of MIP language rules for metaphor detection. Moreover, we\nemploy a teacher model equipped with prior knowledge to generate meaningful\nsoft labels, guiding the optimization process of the student model. The\ninclusion of soft labels, akin to label smoothing, helps alleviate the model's\ntendency towards over-confidence and effectively addresses the challenge of\ndata sparsity. Experimental results demonstrate that our proposed model\nachieves state-of-the-art performance across multiple datasets.\n","authors":["Kaidi Jia","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18251v1","updated":"2024-03-27T04:47:10Z","published":"2024-03-27T04:47:10Z","title":"Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too","summary":"  English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.\n","authors":["Abteen Ebrahimi","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2403.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18249v1","updated":"2024-03-27T04:39:18Z","published":"2024-03-27T04:39:18Z","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges","summary":"  Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.\n","authors":["Yanshen Sun","Jianfeng He","Limeng Cui","Shuo Lei","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14965v4","updated":"2024-03-27T04:38:44Z","published":"2023-05-24T09:57:37Z","title":"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks","summary":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.\n","authors":["Abhinav Rao","Sachin Vashistha","Atharva Naik","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2305.14965v4.pdf","comment":"Accepted at LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18182v1","updated":"2024-03-27T01:19:23Z","published":"2024-03-27T01:19:23Z","title":"ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus","summary":"  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.\n","authors":["Injy Hamed","Fadhl Eryani","David Palfreyman","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2403.18182v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2308.11138v3","updated":"2024-03-27T00:29:33Z","published":"2023-08-22T02:39:42Z","title":"NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints","summary":"  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n","authors":["Peiheng Gao","Ning Sun","Xuefeng Wang","Chen Yang","Ričardas Zitikis"],"pdf_url":"https://arxiv.org/pdf/2308.11138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null}]},"2024-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18152v1","updated":"2024-03-26T23:32:52Z","published":"2024-03-26T23:32:52Z","title":"Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency","summary":"  Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.\n","authors":["Toyin Aguda","Suchetha Siddagangappa","Elena Kochkina","Simerjot Kaur","Dongsheng Wang","Charese Smiley","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.18152v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2310.19055v2","updated":"2024-03-26T22:59:36Z","published":"2023-10-29T16:02:46Z","title":"A Few-Shot Learning Focused Survey on Recent Named Entity Recognition\n  and Relation Classification Methods","summary":"  Named Entity Recognition (NER) and Relation Classification (RC) are important\nsteps in extracting information from unstructured text and formatting it into a\nmachine-readable format. We present a survey of recent deep learning models\nthat address named entity recognition and relation classification, with focus\non few-shot learning performance. Our survey is helpful for researchers in\nknowing the recent techniques in text mining and extracting structured\ninformation from raw text.\n","authors":["Sakher Khalil Alqaaidi","Elika Bozorgi","Afsaneh Shams","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2310.19055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05882v2","updated":"2024-03-26T22:54:48Z","published":"2023-06-09T13:24:27Z","title":"Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems","summary":"  Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n","authors":["Silvia Alma Piazzolla","Beatrice Savoldi","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2306.05882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05677v2","updated":"2024-03-26T22:53:56Z","published":"2023-12-09T20:51:48Z","title":"Batched Low-Rank Adaptation of Foundation Models","summary":"  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.\n","authors":["Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2312.05677v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18125v1","updated":"2024-03-26T22:08:33Z","published":"2024-03-26T22:08:33Z","title":"For those who don't know (how) to ask: Building a dataset of technology\n  questions for digital newcomers","summary":"  While the rise of large language models (LLMs) has created rich new\nopportunities to learn about digital technology, many on the margins of this\ntechnology struggle to gain and maintain competency due to lexical or\nconceptual barriers that prevent them from asking appropriate questions.\nAlthough there have been many efforts to understand factuality of LLM-created\ncontent and ability of LLMs to answer questions, it is not well understood how\nunclear or nonstandard language queries affect the model outputs. We propose\nthe creation of a dataset that captures questions of digital newcomers and\noutsiders, utilizing data we have compiled from a decade's worth of one-on-one\ntutoring. In this paper we lay out our planned efforts and some potential uses\nof this dataset.\n","authors":["Evan Lucas","Kelly S. Steelman","Leo C. Ureel","Charles Wallace"],"pdf_url":"https://arxiv.org/pdf/2403.18125v1.pdf","comment":"Presented at the AI4ED workshop at AAAI 2024"},{"id":"http://arxiv.org/abs/2403.18120v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"  Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.\n","authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18120v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18121v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"ChatGPT Role-play Dataset: Analysis of User Motives and Model\n  Naturalness","summary":"  Recent advances in interactive large language models like ChatGPT have\nrevolutionized various domains; however, their behavior in natural and\nrole-play conversation settings remains underexplored. In our study, we address\nthis gap by deeply investigating how ChatGPT behaves during conversations in\ndifferent settings by analyzing its interactions in both a normal way and a\nrole-play setting. We introduce a novel dataset of broad range of human-AI\nconversations annotated with user motives and model naturalness to examine (i)\nhow humans engage with the conversational AI model, and (ii) how natural are AI\nmodel responses. Our study highlights the diversity of user motives when\ninteracting with ChatGPT and variable AI naturalness, showing not only the\nnuanced dynamics of natural conversations between humans and AI, but also\nproviding new avenues for improving the effectiveness of human-AI\ncommunication.\n","authors":["Yufei Tao","Ameeta Agrawal","Judit Dombi","Tetyana Sydorenko","Jung In Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18121v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.03997v3","updated":"2024-03-26T21:32:51Z","published":"2023-06-06T20:19:33Z","title":"Sentiment Analysis in Finance: From Transformers Back to eXplainable\n  Lexicons (XLex)","summary":"  Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.\n","authors":["Maryan Rizinski","Hristijan Peshov","Kostadin Mishev","Milos Jovanovik","Dimitar Trajanov"],"pdf_url":"https://arxiv.org/pdf/2306.03997v3.pdf","comment":"Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:\n  https://ieeexplore.ieee.org/document/10380556"},{"id":"http://arxiv.org/abs/2403.18105v1","updated":"2024-03-26T21:04:29Z","published":"2024-03-26T21:04:29Z","title":"Large Language Models for Education: A Survey and Outlook","summary":"  The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.\n","authors":["Shen Wang","Tianlong Xu","Hang Li","Chaoli Zhang","Joleen Liang","Jiliang Tang","Philip S. Yu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.18105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18098v1","updated":"2024-03-26T20:47:32Z","published":"2024-03-26T20:47:32Z","title":"GPTs and Language Barrier: A Cross-Lingual Legal QA Examination","summary":"  In this paper, we explore the application of Generative Pre-trained\nTransformers (GPTs) in cross-lingual legal Question-Answering (QA) systems\nusing the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a\nset of related legal articles that serve as context, the objective is to\ndetermine whether the statement is legally valid, i.e., if it can be inferred\nfrom the provided contextual articles or not, which is also known as an\nentailment task. By benchmarking four different combinations of English and\nJapanese prompts and data, we provide valuable insights into GPTs' performance\nin multilingual legal QA scenarios, contributing to the development of more\nefficient and accurate cross-lingual QA solutions in the legal domain.\n","authors":["Ha-Thanh Nguyen","Hiroaki Yamada","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18098v1.pdf","comment":"NLP 2024, Kobe, Japan"},{"id":"http://arxiv.org/abs/2403.18093v1","updated":"2024-03-26T20:25:53Z","published":"2024-03-26T20:25:53Z","title":"Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large\n  Language Models","summary":"  Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.\n","authors":["Hai-Long Nguyen","Duc-Minh Nguyen","Tan-Minh Nguyen","Ha-Thanh Nguyen","Thi-Hai-Yen Vuong","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18093v1.pdf","comment":"JURISIN 2024"},{"id":"http://arxiv.org/abs/2402.09654v2","updated":"2024-03-26T20:12:18Z","published":"2024-02-15T01:38:50Z","title":"GPT-4's assessment of its performance in a USMLE-based case study","summary":"  This study investigates GPT-4's assessment of its performance in healthcare\napplications. A simple prompting technique was used to prompt the LLM with\nquestions taken from the United States Medical Licensing Examination (USMLE)\nquestionnaire and it was tasked to evaluate its confidence score before posing\nthe question and after asking the question. The questionnaire was categorized\ninto two groups-questions with feedback (WF) and questions with no feedback(NF)\npost-question. The model was asked to provide absolute and relative confidence\nscores before and after each question. The experimental findings were analyzed\nusing statistical tools to study the variability of confidence in WF and NF\ngroups. Additionally, a sequential analysis was conducted to observe the\nperformance variation for the WF and NF groups. Results indicate that feedback\ninfluences relative confidence but doesn't consistently increase or decrease\nit. Understanding the performance of LLM is paramount in exploring its utility\nin sensitive areas like healthcare. This study contributes to the ongoing\ndiscourse on the reliability of AI, particularly of LLMs like GPT-4, within\nhealthcare, offering insights into how feedback mechanisms might be optimized\nto enhance AI-assisted medical education and decision support.\n","authors":["Uttam Dhakal","Aniket Kumar Singh","Suman Devkota","Yogesh Sapkota","Bishal Lamichhane","Suprinsa Paudyal","Chandra Dhakal"],"pdf_url":"https://arxiv.org/pdf/2402.09654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12492v2","updated":"2024-03-26T19:28:15Z","published":"2024-01-23T05:20:35Z","title":"Comparing Pre-trained Human Language Models: Is it Better with Human\n  Context as Groups, Individual Traits, or Both?","summary":"  Incorporating human context into language models is the next frontier for\nhuman-centered natural language processing. Currently, two pre-training methods\nexist: group-wise attributes (e.g., over-45-year-olds) or individual traits.\nGroup attributes are coarse -- not all 45-year-olds write the same way -- while\nmodeling individual traits allows for a more personalized representation, but\nrequires more complex modeling and data. So far, it is unclear which\npre-training approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on 5 user- and document-level tasks. We find that pre-training with\nboth group and individual features significantly improves the two user-level\nregression tasks like age estimation and personality assessment. Pre-training\non individual users significantly improves the three document-level\nclassification tasks like stance and topic detection. It even does well for\ndownstream tasks without historical user data. Our results suggest both\napproaches have specific use cases, opening new avenues for human-centered\nlanguage modeling.\n","authors":["Nikita Soni","Niranjan Balasubramanian","H. Andrew Schwartz","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2401.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18058v1","updated":"2024-03-26T19:24:18Z","published":"2024-03-26T19:24:18Z","title":"COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning","summary":"  Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA\n","authors":["Yuelin Bai","Xinrun Du","Yiming Liang","Yonggang Jin","Ziqiang Liu","Junting Zhou","Tianyu Zheng","Xincheng Zhang","Nuo Ma","Zekun Wang","Ruibin Yuan","Haihong Wu","Hongquan Lin","Wenhao Huang","Jiajun Zhang","Wenhu Chen","Chenghua Lin","Jie Fu","Min Yang","Shiwen Ni","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18051v1","updated":"2024-03-26T19:08:20Z","published":"2024-03-26T19:08:20Z","title":"Supervisory Prompt Training","summary":"  The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.\n","authors":["Jean Ghislain Billa","Min Oh","Liang Du"],"pdf_url":"https://arxiv.org/pdf/2403.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18031v1","updated":"2024-03-26T18:38:14Z","published":"2024-03-26T18:38:14Z","title":"The Impact of Syntactic and Semantic Proximity on Machine Translation\n  with Back-Translation","summary":"  Unsupervised on-the-fly back-translation, in conjunction with multilingual\npretraining, is the dominant method for unsupervised neural machine\ntranslation. Theoretically, however, the method should not work in general. We\ntherefore conduct controlled experiments with artificial languages to determine\nwhat properties of languages make back-translation an effective training\nmethod, covering lexical, syntactic, and semantic properties. We find, contrary\nto popular belief, that (i) parallel word frequency distributions, (ii)\npartially shared vocabulary, and (iii) similar syntactic structure across\nlanguages are not sufficient to explain the success of back-translation. We\nshow however that even crude semantic signal (similar lexical fields across\nlanguages) does improve alignment of two languages through back-translation. We\nconjecture that rich semantic dependencies, parallel across languages, are at\nthe root of the success of unsupervised methods based on back-translation.\nOverall, the success of unsupervised machine translation was far from being\nanalytically guaranteed. Instead, it is another proof that languages of the\nworld share deep similarities, and we hope to show how to identify which of\nthese similarities can serve the development of unsupervised, cross-linguistic\ntools.\n","authors":["Nicolas Guerin","Shane Steinert-Threlkeld","Emmanuel Chemla"],"pdf_url":"https://arxiv.org/pdf/2403.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10438v6","updated":"2024-03-26T18:36:31Z","published":"2022-11-18T18:59:33Z","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models","summary":"  Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.\n","authors":["Guangxuan Xiao","Ji Lin","Mickael Seznec","Hao Wu","Julien Demouth","Song Han"],"pdf_url":"https://arxiv.org/pdf/2211.10438v6.pdf","comment":"ICML 2023. First two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.18024v1","updated":"2024-03-26T18:22:05Z","published":"2024-03-26T18:22:05Z","title":"Enriching Word Usage Graphs with Cluster Definitions","summary":"  We present a dataset of word usage graphs (WUGs), where the existing WUGs for\nmultiple languages are enriched with cluster labels functioning as sense\ndefinitions. They are generated from scratch by fine-tuned encoder-decoder\nlanguage models. The conducted human evaluation has shown that these\ndefinitions match the existing clusters in WUGs better than the definitions\nchosen from WordNet by two baseline systems. At the same time, the method is\nstraightforward to use and easy to extend to new languages. The resulting\nenriched datasets can be extremely helpful for moving on to explainable\nsemantic change modeling.\n","authors":["Mariia Fedorova","Andrey Kutuzov","Nikolay Arefyev","Dominik Schlechtweg"],"pdf_url":"https://arxiv.org/pdf/2403.18024v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14814v2","updated":"2024-03-26T18:10:10Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v2.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.18018v1","updated":"2024-03-26T18:07:10Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2305.14718v4","updated":"2024-03-26T18:07:01Z","published":"2023-05-24T04:42:17Z","title":"Leftover-Lunch: Advantage-based Offline Reinforcement Learning for\n  Language Models","summary":"  Reinforcement Learning with Human Feedback (RLHF) is the most prominent\nmethod for Language Model (LM) alignment. However, RLHF is an unstable and\ndata-hungry process that continually requires new high-quality LM-generated\ndata for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new\nclass of offline policy gradient algorithms that enable RL training on any\npre-existing data. By assuming the entire LM output sequence as a single\naction, A-LoL allows incorporating sequence-level classifiers or human-designed\nscoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL\nonly trains on positive advantage (leftover) data points, making it resilient\nto noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable\nLM training recipe.\n  We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than the baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data.\n  We also release our experimental code. https://github.com/abaheti95/LoL-RL\n","authors":["Ashutosh Baheti","Ximing Lu","Faeze Brahman","Ronan Le Bras","Maarten Sap","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2305.14718v4.pdf","comment":"published at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16248v2","updated":"2024-03-26T17:46:26Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17860v1","updated":"2024-03-26T16:49:25Z","published":"2024-03-26T16:49:25Z","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to\n  Minimize High Confidence Misclassifications","summary":"  Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17859v1","updated":"2024-03-26T16:48:13Z","published":"2024-03-26T16:48:13Z","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on\n  Historical American Newspaper Pages","summary":"  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.\n","authors":["Bhawna Piryani","Jamshid Mozafari","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17859v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17856v1","updated":"2024-03-26T16:45:27Z","published":"2024-03-26T16:45:27Z","title":"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation\n  in Five LLMs","summary":"  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n","authors":["David R. Mortensen","Valentina Izrailevitch","Yunze Xiao","Hinrich Schütze","Leonie Weissweiler"],"pdf_url":"https://arxiv.org/pdf/2403.17856v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11278v3","updated":"2024-03-26T16:40:50Z","published":"2023-07-21T00:34:38Z","title":"Generator-Retriever-Generator Approach for Open-Domain Question\n  Answering","summary":"  Open-domain question answering (QA) tasks usually require the retrieval of\nrelevant information from a large corpus to generate accurate answers. We\npropose a novel approach called Generator-Retriever-Generator (GRG) that\ncombines document retrieval techniques with a large language model (LLM), by\nfirst prompting the model to generate contextual documents based on a given\nquestion. In parallel, a dual-encoder network retrieves documents that are\nrelevant to the question from an external corpus. The generated and retrieved\ndocuments are then passed to the second LLM, which generates the final answer.\nBy combining document retrieval and LLM generation, our approach addresses the\nchallenges of open-domain QA, such as generating informative and contextually\nrelevant answers. GRG outperforms the state-of-the-art generate-then-read and\nretrieve-then-read pipelines (GENREAD and RFiD) improving their performance by\nat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,\nrespectively. We provide code, datasets, and checkpoints at\nhttps://github.com/abdoelsayed2016/GRG.\n","authors":["Abdelrahman Abdallah","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2307.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2309.09800v3","updated":"2024-03-26T16:05:51Z","published":"2023-09-18T14:18:19Z","title":"AMuRD: Annotated Arabic-English Receipt Dataset for Key Information\n  Extraction and Classification","summary":"  The extraction of key information from receipts is a complex task that\ninvolves the recognition and extraction of text from scanned receipts. This\nprocess is crucial as it enables the retrieval of essential content and\norganizing it into structured documents for easy access and analysis. In this\npaper, we present AMuRD, a novel multilingual human-annotated dataset\nspecifically designed for information extraction from receipts. This dataset\ncomprises $47,720$ samples and addresses the key challenges in information\nextraction and item classification - the two critical aspects of data analysis\nin the retail industry. Each sample includes annotations for item names and\nattributes such as price, brand, and more. This detailed annotation facilitates\na comprehensive understanding of each item on the receipt. Furthermore, the\ndataset provides classification into $44$ distinct product categories. This\nclassification feature allows for a more organized and efficient analysis of\nthe items, enhancing the usability of the dataset for various applications. In\nour study, we evaluated various language model architectures, e.g., by\nfine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional\nresults, with an F1 score of 97.43\\% and accuracy of 94.99\\% in information\nextraction and classification, and an even higher F1 score of 98.51\\% and\naccuracy of 97.06\\% observed in specific tasks. The dataset and code are\npublicly accessible for further\nresearchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.\n","authors":["Abdelrahman Abdallah","Mahmoud Abdalla","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2309.09800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03742v2","updated":"2024-03-26T16:03:57Z","published":"2023-08-07T17:46:49Z","title":"Training BERT Models to Carry Over a Coding System Developed on One\n  Corpus to Another","summary":"  This paper describes how we train BERT models to carry over a coding system\ndeveloped on the paragraphs of a Hungarian literary journal to another. The aim\nof the coding system is to track trends in the perception of literary\ntranslation around the political transformation in 1989 in Hungary. To evaluate\nnot only task performance but also the consistence of the annotation, moreover,\nto get better predictions from an ensemble, we use 10-fold crossvalidation.\nExtensive hyperparameter tuning is used to obtain the best possible results and\nfair comparisons. To handle label imbalance, we use loss functions and metrics\nrobust to it. Evaluation of the effect of domain shift is carried out by\nsampling a test set from the target domain. We establish the sample size by\nestimating the bootstrapped confidence interval via simulations. This way, we\nshow that our models can carry over one annotation system to the target domain.\nComparisons are drawn to provide insights such as learning multilabel\ncorrelations and confidence penalty improve resistance to domain shift, and\ndomain adaptation on OCR-ed text on another domain improves performance almost\nto the same extent as that on the corpus under study. See our code at\nhttps://codeberg.org/zsamboki/bert-annotator-ensemble.\n","authors":["Dalma Galambos","Pál Zsámboki"],"pdf_url":"https://arxiv.org/pdf/2308.03742v2.pdf","comment":"Camera-ready version, to be presented at the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17816v1","updated":"2024-03-26T15:53:02Z","published":"2024-03-26T15:53:02Z","title":"Graph Language Model (GLM): A new graph-based approach to detect social\n  instabilities","summary":"  This scientific report presents a novel methodology for the early prediction\nof important political events using News datasets. The methodology leverages\nnatural language processing, graph theory, clique analysis, and semantic\nrelationships to uncover hidden predictive signals within the data. Initially,\nwe designed a preliminary version of the method and tested it on a few events.\nThis analysis revealed limitations in the initial research phase. We then\nenhanced the model in two key ways: first, we added a filtration step to only\nconsider politically relevant news before further processing; second, we\nadjusted the input features to make the alert system more sensitive to\nsignificant spikes in the data. After finalizing the improved methodology, we\ntested it on eleven events including US protests, the Ukraine war, and French\nprotests. Results demonstrate the superiority of our approach compared to\nbaseline methods. Through targeted refinements, our model can now provide\nearlier and more accurate predictions of major political events based on subtle\npatterns in news data.\n","authors":["Wallyson Lemes de Oliveira","Vahid Shamsaddini","Ali Ghofrani","Rahul Singh Inda","Jithendra Sai Veeramaneni","Étienne Voutaz"],"pdf_url":"https://arxiv.org/pdf/2403.17816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2401.11911v4","updated":"2024-03-26T15:47:14Z","published":"2024-01-22T12:54:04Z","title":"Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?","summary":"  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n","authors":["Hexiang Tan","Fei Sun","Wanli Yang","Yuanzhuo Wang","Qi Cao","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01748v2","updated":"2024-03-26T15:26:21Z","published":"2024-03-04T05:55:01Z","title":"Decode Neural signal as Speech","summary":"  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n","authors":["Yiqian Yang","Yiqun Duan","Qiang Zhang","Renjing Xu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.01748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17760v1","updated":"2024-03-26T14:51:12Z","published":"2024-03-26T14:51:12Z","title":"Constructions Are So Difficult That Even Large Language Models Get Them\n  Right for the Wrong Reasons","summary":"  In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.\n","authors":["Shijia Zhou","Leonie Weissweiler","Taiqi He","Hinrich Schütze","David R. Mortensen","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.17760v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17752v1","updated":"2024-03-26T14:43:48Z","published":"2024-03-26T14:43:48Z","title":"Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?","summary":"  Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.\n","authors":["Wangyue Li","Liangzhi Li","Tong Xiang","Xiao Liu","Wei Deng","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.17752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17748v1","updated":"2024-03-26T14:40:10Z","published":"2024-03-26T14:40:10Z","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal\n  Dependencies","summary":"  The Universal Dependencies (UD) project has created an invaluable collection\nof treebanks with contributions in over 140 languages. However, the UD\nannotations do not tell the full story. Grammatical constructions that convey\nmeaning through a particular combination of several morphosyntactic elements --\nfor example, interrogative sentences with special markers and/or word orders --\nare not labeled holistically. We argue for (i) augmenting UD annotations with a\n'UCxn' annotation layer for such meaning-bearing grammatical constructions, and\n(ii) approaching this in a typologically informed way so that morphosyntactic\nstrategies can be compared across languages. As a case study, we consider five\nconstruction families in ten languages, identifying instances of each\nconstruction in UD treebanks through the use of morphosyntactic patterns. In\naddition to findings regarding these particular constructions, our study yields\nimportant insights on methodology for describing and identifying constructions\nin language-general and language-particular ways, and lays the foundation for\nfuture constructional enrichment of UD treebanks.\n","authors":["Leonie Weissweiler","Nina Böbel","Kirian Guiller","Santiago Herrera","Wesley Scivetti","Arthur Lorenzi","Nurit Melnik","Archna Bhatia","Hinrich Schütze","Lori Levin","Amir Zeldes","Joakim Nivre","William Croft","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17748v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"}]}}